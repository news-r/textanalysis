---
title: "Get Started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Get Started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Setup

This document will walk you through the general concepts of textanalysis and demonstrate the broad workflow of the package. First you will need to have the package installed of course; instructions are on the [homepage](https://textanalysis.news-r.org). Once installed, the package can be loaded.

```{r}
library(textanalysis)
```

Because the package depends on Julia we _must_ initialise the session.

```{r}
init_textanalysis()
```

## Document

Once the pacakges, installed, loaded, and the session initialised we can start using the package. The most basic object type in textanalysis is the document, it can be created with any of the `*_document` functions, though you will likely only need `string_document` to create a document from a character string.

```{r}
str <- "This is a very simple document!"
(doc <- string_document(str))
```

You can always get the content of the document with `get_text`.

```{r}
get_text(doc)
```

Turning a character string into a `document` allows to easily clean it, or `prepare` it in textanalysis jargon. There are multitude of ways to clean text in textanalysis, they are further detailed in the [preprocessing vignettes](articles/prepare.html). Here we use the straightforward `prepare` function with leaving all arguments on default.

```{r}
prepare(doc)
```

Notice the warning, the prepare function changes the object `doc` _in place_; we did not assign the (inexistent) output of `prepare` to a new object and the object `doc` changed. Let's demonstrate.

```{r}
get_text(doc) # see the document changed!
```

This is somewhat puzzling for us R users but it actually happens for good reasons: textanalysis does not need to make a copy of the object, this allows processing more data.

However, you will not use the package in this manner as you have multiple documents to process at once, you can do so with the `to_documents` function which will process multiple documents from a vector or data.frame. All functions previously used also work on objects of class `documents`. 

```{r}
texts <- c(
  str,
  "This is another document"
)

(docs <- to_documents(texts))

prepare(docs)
get_text(docs)
```

## Corpus

With our documents constructed and clean we can build a corpus of our documents. You can do so with `corpus(docs)`, however we will do so with a dataset of the package: a set of 1,045 Reuters articles on 10 different commodities.

```{r}
data("reuters")

dplyr::count(reuters, category, sort = TRUE)
```

Since we 

```{r}
docs <- to_documents(reuters, text = text)
prepare(docs)
(crps <- corpus(docs))
```

We can already do a lot more with a corpus than mere documents, like extracting the lexicon, computing its size (number of unique words), and more.

```{r}
lexicon_size(crps)
lexicon <- lexicon(crps)

lexicon %>% 
  dplyr::arrange(-n) %>% 
  dplyr::slice(1:5)
```

You could also fetch thse lexical frequency of a specific word.

```{r}
lexical_frequency(crps, "corn")
```

## Document-term Matrix

Now that we have a corpus we can compute the document-term matrix, which is the core of a multitude of computations and models.

```{r}
(dtm <- document_term_matrix(crps))
```
