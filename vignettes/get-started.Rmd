---
title: "Get Started"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Get Started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Setup

This document will walk you through the general concepts of textanalysis and demonstrate the broad workflow of the package. First you will need to have the package installed of course; instructions are on the [homepage](https://textanalysis.news-r.org). Once installed, the package can be loaded.

```{r}
library(textanalysis)
```

Because the package depends on Julia we _must_ initialise the session.

```{r}
init_textanalysis()
```

## Document

Once the pacakges, installed, loaded, and the session initialised we can start using the package. The most basic object type in textanalysis is the document, it can be created with any of the `*_document` functions, though you will likely only need `string_document` to create a document from a character string.

```{r}
str <- "This is a very simple document!"
(doc <- string_document(str))
```

You can always get the content of the document with `get_text`.

```{r}
get_text(doc)
```

Turning a character string into a `document` allows to easily clean it, or `prepare` it in textanalysis jargon. There are multitude of ways to clean text in textanalysis, they are further detailed in the [preprocessing vignettes](articles/prepare.html). Here we use the straightforward `prepare` function with leaving all arguments on default.

```{r}
prepare(doc)
```

Notice the warning, the prepare function changes the object `doc` _in place_; we did not assign the (inexistent) output of `prepare` to a new object and the object `doc` changed. Let's demonstrate.

```{r}
get_text(doc) # see the document changed!
```

This is somewhat puzzling for us R users but it actually happens for good reasons: textanalysis does not need to make a copy of the object, this allows processing more data.

However, you will not use the package in this manner as you have multiple documents to process at once, you can do so with the `to_documents` function which will process multiple documents from a vector or data.frame. All functions previously used also work on objects of class `documents`. 

```{r}
texts <- c(
  str,
  "This is another document"
)

(docs <- to_documents(texts))

prepare(docs)
get_text(docs)
```

## Corpus

With our documents constructed and clean we can build a corpus of our documents. You can do so with `corpus(docs)`, however we will do so with a sample of dataset of the package: a set of 1,045 Reuters articles on 10 different commodities.

```{r}
data("reuters")

dplyr::count(reuters, category, sort = TRUE)
```

We'll use the function `to_documents` which has methods for character vectors and data.frame, to build multiple documents at once, we'll just take 3 documents.

```{r}
docs <- reuters %>%
  dplyr::slice(1:3) %>% 
  to_documents(text = text)
prepare(docs)
(crps <- corpus(docs))
```

We can already do a lot more with a corpus than mere documents, like extracting the lexicon, computing its size (number of unique words), and more.

```{r}
lexicon_size(crps)
lexicon <- lexicon(crps)

lexicon %>% 
  dplyr::arrange(-n) %>% 
  dplyr::slice(1:5)
```

You could also fetch thse lexical frequency of a specific word.

```{r}
lexical_frequency(crps, "corn")
```

You can even assess the sentiment of documents.

```{r}
sentiment(crps)
```

## Document-term Matrix

Now that we have a corpus we can compute the document-term matrix, which is the core of a multitude of computations and models.

```{r}
(dtm <- document_term_matrix(crps))
```

A document-term matrix will enable you to compute various frequency-related matrices, like tf-idf (term-frequency inverse document frequency), or even Okapi BM-25.

```{r}
tf <- tf(dtm)
tfidf <- tf_idf(dtm)
okapi <- bm_25(dtm)
```

Note that the functions ran above do not return the words themselves, you can obtain these with the `lexicon` function. This could be done with a function such as `bind_lexicon` declared below: very straightforward, simply add a column for the words (lexicon).

```{r}
# function to bind the lexicon to our matrices
bind_lexicon <- function(data){
  data %>% 
    as.data.frame() %>% 
    dplyr::bind_cols(
      lexicon %>% 
        dplyr::select(-n),
      .
    )
}

bind_lexicon(tf)
bind_lexicon(tfidf)
bind_lexicon(okapi)
```
