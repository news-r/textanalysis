---
output: 
  github_document:
    html_preview: false
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

<!-- badges: start -->
[![Travis build status](https://travis-ci.org/news-r/textanalysis.svg?branch=master)](https://travis-ci.org/news-r/textanalysis)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
<!-- badges: end -->

# textanalysis

Text Analysis in R via Julia.

## Installation

Being a wrapper to a [Julia](https://julialang.org/) package, textanalysis requires the latter to be installed.

```{r, eval=FALSE}
# install.packages("remotes")
remotes::install_github("news-r/textanalysis") # github
```

## Setup

You _must_ run `init_textanalysis` at the begining of every session, you will otherwise encounter errors and be prompted to do so.

```{r}
textanalysis::init_textanalysis() # setup textanalysis Julia dependency
```

## Example

```{r}
library(textanalysis)

# build document
str <- paste(
  "They <span>write</span>, it writes too!!!",
  "This is another sentence.",
  "More stuff in this document."
)
doc <- string_document(str)

# basic cleanup
prepare(doc)
get_text(doc)

# stem
stem_words(doc)
get_text(doc)

# corpus
doc2 <- token_document("Hey write another document.")
get_text(doc2)

# combine
corpus <- corpus(doc, doc2)

# standardize
standardize(corpus, "token_document")

# prepare corpus
prepare(corpus, strip_html_tags = FALSE)
get_text(corpus[[1]])

# lexicon + lexical stats
update_lexicon(corpus)
lexicon(corpus)
lexical_frequency(corpus, "document")

# inverse index
update_inverse_index(corpus)
inverse_index(corpus)

# dtm
m <- document_term_matrix(corpus)

# term-frequency
tf(m)

# tf-idf
tf_idf(m)

# sentiment
sentiment(corpus)

# summarise in 2 sentences
summarize(string_document(str), ns = 2L)

# lda 2 topics
lda(m, 2L, 1000L)
```
